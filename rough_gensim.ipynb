{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e91c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/site-packages (from gensim) (1.22.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed gensim-4.2.0 smart-open-6.0.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac134d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import open\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af162fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a document\n",
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcf8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f6a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining stop words\n",
    "stoplist = set('for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b8f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating through the documents in the corpus and lowercasing the words and excluding the stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "# list of lists of individual tokens of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d57d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the qord frequencies\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    #print(text)\n",
    "    for token in text:\n",
    "        #print(token)\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2adc472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "#only keeping words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token]>1]for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c19981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "#creating a vocabulary of the words present in the corpus\n",
    "#One need to feed the tokens got out of the corpus to the library function\n",
    "#dictionary is then saved\n",
    "vocabulary = corpora.Dictionary(processed_corpus)\n",
    "vocabulary.save('deerwester.dict')\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fcab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "#printing the vocabulary with tokens and their id\n",
    "pprint.pprint(vocabulary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dfef854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (1, 2), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "#printing the (id,freq) of the words in a new sentence\n",
    "#if a word is not present in the vocabulary no output regarding the same is given\n",
    "new_doc = \"Human Human computer computer computer eps interaction\"\n",
    "new_vec = vocabulary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f052f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#creating the bag of words for our corpus\n",
    "#this replaces the tokens of the documents with (id,freq) pairs\n",
    "#frequency is with respect to the document and not the corpus\n",
    "bow_corpus = [vocabulary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8dc9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "#tfidf model simply replaces the frequency with the rarity parameter of the word in the corpus\n",
    "# one can see that \"minors\" is rare than \"system\" in our corpus thus more weight is assigned to \"minors\"\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[vocabulary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e011c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a similarity query\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fcc8e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "#outputting the similarity of each of the document with th equery document\n",
    "#note that we need to first convert the query doc to bag of words and apply the model to it\n",
    "#then only we can find the similarity, kind of obvious but still mentioned it\n",
    "query_doc = \"system engineering\".split()\n",
    "query_bow = vocabulary.doc2bow(query_doc)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a00cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f982766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "141b94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can parse each document at a time as follows\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('https://radimrehurek.com/mycorpus.txt'):\n",
    "            yield vocabulary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256de3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x7fe642191f70>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e691c1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ab1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collect statistics about all tokens\n",
    "# dictionary = corpora.Dictionary(line.lower().split() for line in open('https://radimrehurek.com/mycorpus.txt'))\n",
    "# # remove stop words and words that appear only once\n",
    "# stop_ids = [\n",
    "#     dictionary.token2id[stopword]\n",
    "#     for stopword in stoplist\n",
    "#     if stopword in dictionary.token2id\n",
    "# ]\n",
    "# once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "# dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "# dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be2582d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 15:48:05,775 : INFO : loading projection weights from /Users/shri/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2022-05-29 15:48:49,833 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/shri/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-05-29T15:48:49.833364', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "#working with pretrained word2vec model based on google news\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d6fbf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "#geeting thw vocabulary of the model\n",
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index==10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e8ab6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the embedding of the word king\n",
    "wv['king'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59b27c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found\n"
     ]
    }
   ],
   "source": [
    "#check if the word \"cameroon\" is present in the vocab or not\n",
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f841bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "#outputting the similarity measures of the given pair of words\n",
    "pairs = [\n",
    "    ('car', 'minivan'),  \n",
    "    ('car', 'bicycle'),   \n",
    "    ('car', 'airplane'),  \n",
    "    ('car', 'cereal'),    \n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29c3655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training your own model\n",
    "#we will feed our corpus to the class to get it preprocessed\n",
    "#this will comvert to bag of words\n",
    "#then we will feed this bow to model and will get a trained model for this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d37c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus1:\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af8d5ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 16:08:59,732 : INFO : collecting all words and their counts\n",
      "2022-05-29 16:08:59,734 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-29 16:08:59,825 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2022-05-29 16:08:59,826 : INFO : Creating a fresh vocabulary\n",
      "2022-05-29 16:08:59,835 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.068041827818366%% of original 6981, drops 5231)', 'datetime': '2022-05-29T16:08:59.835337', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:08:59,836 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.83801073049938%% of original 58152, drops 8817)', 'datetime': '2022-05-29T16:08:59.836257', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:08:59,849 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2022-05-29 16:08:59,850 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2022-05-29 16:08:59,850 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2022-05-29T16:08:59.850715', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:08:59,872 : INFO : estimated required memory for 1750 words and 100 dimensions: 2275000 bytes\n",
      "2022-05-29 16:08:59,872 : INFO : resetting layer weights\n",
      "2022-05-29 16:08:59,874 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-29T16:08:59.874945', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-05-29 16:08:59,875 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-05-29T16:08:59.875726', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:08:59,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:08:59,979 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:08:59,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:08:59,986 : INFO : EPOCH - 1 : training on 58152 raw words (35945 effective words) took 0.1s, 329706 effective words/s\n",
      "2022-05-29 16:09:00,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:09:00,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:09:00,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:09:00,096 : INFO : EPOCH - 2 : training on 58152 raw words (35941 effective words) took 0.1s, 334113 effective words/s\n",
      "2022-05-29 16:09:00,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:09:00,190 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:09:00,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:09:00,194 : INFO : EPOCH - 3 : training on 58152 raw words (35969 effective words) took 0.1s, 373804 effective words/s\n",
      "2022-05-29 16:09:00,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:09:00,292 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:09:00,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:09:00,297 : INFO : EPOCH - 4 : training on 58152 raw words (35926 effective words) took 0.1s, 355011 effective words/s\n",
      "2022-05-29 16:09:00,390 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:09:00,390 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:09:00,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:09:00,396 : INFO : EPOCH - 5 : training on 58152 raw words (35910 effective words) took 0.1s, 397959 effective words/s\n",
      "2022-05-29 16:09:00,396 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179691 effective words) took 0.5s, 345471 effective words/s', 'datetime': '2022-05-29T16:09:00.396437', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:09:00,396 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=1750, vector_size=100, alpha=0.025)', 'datetime': '2022-05-29T16:09:00.396839', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "sentences = MyCorpus1()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f09028",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f78063e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02106759  0.06429333  0.01470954  0.0174621   0.01274141 -0.11490468\n",
      "  0.04796187  0.12098181 -0.00810785 -0.0211865  -0.00792687 -0.07437152\n",
      "  0.00718535  0.03965204  0.00878629  0.01540343 -0.00242962 -0.00237106\n",
      " -0.02652099 -0.0863167   0.05015018  0.01119104  0.01777221 -0.00151447\n",
      " -0.02835938  0.02660489 -0.02678897 -0.01507284 -0.03924191  0.01898382\n",
      "  0.04685591 -0.05614002  0.05111133 -0.04416491 -0.01295188  0.06633321\n",
      "  0.02150068  0.01328949 -0.02671723 -0.04253641 -0.0198428   0.00500556\n",
      " -0.0143372   0.01908546  0.03790782 -0.02720901 -0.03607159 -0.00428732\n",
      "  0.01140022  0.04330159  0.0226995  -0.03364299 -0.02334993 -0.00048908\n",
      " -0.02178952  0.02385082 -0.00028942  0.00100566 -0.03284542  0.00335183\n",
      " -0.01953491  0.00039112  0.01302968 -0.00411065 -0.0403197   0.0826552\n",
      "  0.02162331  0.04560452 -0.05519556  0.06469066 -0.01416294  0.00754313\n",
      "  0.06652675 -0.01037265  0.05146144  0.03764157 -0.00167219 -0.02792636\n",
      " -0.05436959 -0.01833669 -0.04436235  0.01094308 -0.04541027  0.05417696\n",
      " -0.0033142  -0.01723999  0.02968969  0.04479513  0.06656134  0.01632947\n",
      "  0.0508521   0.07206775  0.06083026  0.0153475   0.12552813  0.02836015\n",
      "  0.06298843 -0.00757825  0.00924084 -0.0080159 ]\n"
     ]
    }
   ],
   "source": [
    "print(vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f27656eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "#printing the vocabulary\n",
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index==10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "237b333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 16:10:53,612 : INFO : Word2Vec lifecycle event {'fname_or_handle': '/var/folders/21/3qr5by0s2yd2xqyy0hshr3kh0000gp/T/gensim-model-wm_okd3k', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-05-29T16:10:53.612865', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-05-29 16:10:53,613 : INFO : not storing attribute cum_table\n",
      "2022-05-29 16:10:53,617 : INFO : saved /var/folders/21/3qr5by0s2yd2xqyy0hshr3kh0000gp/T/gensim-model-wm_okd3k\n",
      "2022-05-29 16:10:53,618 : INFO : loading Word2Vec object from /var/folders/21/3qr5by0s2yd2xqyy0hshr3kh0000gp/T/gensim-model-wm_okd3k\n",
      "2022-05-29 16:10:53,621 : INFO : loading wv recursively from /var/folders/21/3qr5by0s2yd2xqyy0hshr3kh0000gp/T/gensim-model-wm_okd3k.wv.* with mmap=None\n",
      "2022-05-29 16:10:53,622 : INFO : setting ignored attribute cum_table to None\n",
      "2022-05-29 16:10:53,644 : INFO : Word2Vec lifecycle event {'fname': '/var/folders/21/3qr5by0s2yd2xqyy0hshr3kh0000gp/T/gensim-model-wm_okd3k', 'datetime': '2022-05-29T16:10:53.644861', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#saving the model\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53bfd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1cc1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum count \n",
    "#Kep on ly those words that has count atleast equal to min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac8ea534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 16:15:35,331 : INFO : collecting all words and their counts\n",
      "2022-05-29 16:15:35,334 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-29 16:15:35,434 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2022-05-29 16:15:35,435 : INFO : Creating a fresh vocabulary\n",
      "2022-05-29 16:15:35,441 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 889 unique words (12.734565248531728%% of original 6981, drops 6092)', 'datetime': '2022-05-29T16:15:35.441464', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:15:35,442 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 43776 word corpus (75.2785802723896%% of original 58152, drops 14376)', 'datetime': '2022-05-29T16:15:35.442341', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:15:35,449 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2022-05-29 16:15:35,450 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2022-05-29 16:15:35,450 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 29691.39528319831 word corpus (67.8%% of prior 43776)', 'datetime': '2022-05-29T16:15:35.450747', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:15:35,462 : INFO : estimated required memory for 889 words and 100 dimensions: 1155700 bytes\n",
      "2022-05-29 16:15:35,463 : INFO : resetting layer weights\n",
      "2022-05-29 16:15:35,464 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-29T16:15:35.464768', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-05-29 16:15:35,465 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 889 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-05-29T16:15:35.465449', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:15:35,582 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:15:35,586 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:15:35,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:15:35,590 : INFO : EPOCH - 1 : training on 58152 raw words (29735 effective words) took 0.1s, 242401 effective words/s\n",
      "2022-05-29 16:15:35,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:15:35,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:15:35,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:15:35,700 : INFO : EPOCH - 2 : training on 58152 raw words (29771 effective words) took 0.1s, 274289 effective words/s\n",
      "2022-05-29 16:15:35,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:15:35,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:15:35,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:15:35,815 : INFO : EPOCH - 3 : training on 58152 raw words (29689 effective words) took 0.1s, 263543 effective words/s\n",
      "2022-05-29 16:15:35,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:15:35,927 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:15:35,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:15:35,933 : INFO : EPOCH - 4 : training on 58152 raw words (29606 effective words) took 0.1s, 255070 effective words/s\n",
      "2022-05-29 16:15:36,050 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:15:36,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:15:36,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:15:36,057 : INFO : EPOCH - 5 : training on 58152 raw words (29670 effective words) took 0.1s, 284442 effective words/s\n",
      "2022-05-29 16:15:36,057 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (148471 effective words) took 0.6s, 251098 effective words/s', 'datetime': '2022-05-29T16:15:36.057671', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:15:36,058 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=889, vector_size=100, alpha=0.025)', 'datetime': '2022-05-29T16:15:36.058220', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08853aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 16:19:07,134 : INFO : collecting all words and their counts\n",
      "2022-05-29 16:19:07,137 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-05-29 16:19:07,229 : INFO : collected 6981 word types from a corpus of 58152 raw words and 300 sentences\n",
      "2022-05-29 16:19:07,230 : INFO : Creating a fresh vocabulary\n",
      "2022-05-29 16:19:07,239 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1750 unique words (25.068041827818366%% of original 6981, drops 5231)', 'datetime': '2022-05-29T16:19:07.239016', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:19:07,240 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49335 word corpus (84.83801073049938%% of original 58152, drops 8817)', 'datetime': '2022-05-29T16:19:07.239984', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:19:07,252 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2022-05-29 16:19:07,253 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2022-05-29 16:19:07,254 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35935.33721568072 word corpus (72.8%% of prior 49335)', 'datetime': '2022-05-29T16:19:07.253997', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-05-29 16:19:07,277 : INFO : estimated required memory for 1750 words and 200 dimensions: 3675000 bytes\n",
      "2022-05-29 16:19:07,277 : INFO : resetting layer weights\n",
      "2022-05-29 16:19:07,280 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-05-29T16:19:07.280154', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-05-29 16:19:07,280 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1750 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-05-29T16:19:07.280943', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:19:07,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:19:07,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:19:07,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:19:07,390 : INFO : EPOCH - 1 : training on 58152 raw words (35875 effective words) took 0.1s, 333860 effective words/s\n",
      "2022-05-29 16:19:07,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:19:07,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:19:07,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:19:07,496 : INFO : EPOCH - 2 : training on 58152 raw words (35920 effective words) took 0.1s, 345573 effective words/s\n",
      "2022-05-29 16:19:07,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:19:07,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:19:07,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:19:07,600 : INFO : EPOCH - 3 : training on 58152 raw words (36011 effective words) took 0.1s, 352541 effective words/s\n",
      "2022-05-29 16:19:07,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:19:07,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:19:07,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:19:07,706 : INFO : EPOCH - 4 : training on 58152 raw words (35912 effective words) took 0.1s, 350114 effective words/s\n",
      "2022-05-29 16:19:07,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-05-29 16:19:07,810 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-05-29 16:19:07,813 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-05-29 16:19:07,814 : INFO : EPOCH - 5 : training on 58152 raw words (35936 effective words) took 0.1s, 340819 effective words/s\n",
      "2022-05-29 16:19:07,814 : INFO : Word2Vec lifecycle event {'msg': 'training on 290760 raw words (179654 effective words) took 0.5s, 336927 effective words/s', 'datetime': '2022-05-29T16:19:07.814746', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-05-29 16:19:07,815 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=1750, vector_size=200, alpha=0.025)', 'datetime': '2022-05-29T16:19:07.815511', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "#length of word vectors\n",
    "#large vectors results in better model with a cost of \n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=200, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c540dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of workers\n",
    "#used for training parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5930537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 16:22:00,573 : INFO : Evaluating word analogies for top 300000 words in the model on /Users/shri/opt/anaconda3/lib/python3.9/site-packages/gensim/test/test_data/questions-words.txt\n",
      "2022-05-29 16:22:00,588 : INFO : capital-common-countries: 0.0% (0/6)\n",
      "2022-05-29 16:22:00,605 : INFO : capital-world: 0.0% (0/2)\n",
      "2022-05-29 16:22:00,621 : INFO : family: 0.0% (0/6)\n",
      "2022-05-29 16:22:00,638 : INFO : gram3-comparative: 0.0% (0/20)\n",
      "2022-05-29 16:22:00,647 : INFO : gram4-superlative: 0.0% (0/12)\n",
      "2022-05-29 16:22:00,656 : INFO : gram5-present-participle: 0.0% (0/20)\n",
      "2022-05-29 16:22:00,670 : INFO : gram6-nationality-adjective: 3.3% (1/30)\n",
      "2022-05-29 16:22:00,682 : INFO : gram7-past-tense: 0.0% (0/20)\n",
      "2022-05-29 16:22:00,696 : INFO : gram8-plural: 0.0% (0/30)\n",
      "2022-05-29 16:22:00,701 : INFO : Quadruplets with out-of-vocabulary words: 99.3%\n",
      "2022-05-29 16:22:00,702 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2022-05-29 16:22:00,703 : INFO : Total accuracy: 0.7% (1/146)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00684931506849315,\n",
       " [{'section': 'capital-common-countries',\n",
       "   'correct': [],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]},\n",
       "  {'section': 'capital-world',\n",
       "   'correct': [],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]},\n",
       "  {'section': 'currency', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'family',\n",
       "   'correct': [],\n",
       "   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
       "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HIS', 'HER')]},\n",
       "  {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'gram3-comparative',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'LOW', 'LOWER')]},\n",
       "  {'section': 'gram4-superlative',\n",
       "   'correct': [],\n",
       "   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]},\n",
       "  {'section': 'gram5-present-participle',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'RUN', 'RUNNING')]},\n",
       "  {'section': 'gram6-nationality-adjective',\n",
       "   'correct': [('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN')],\n",
       "   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]},\n",
       "  {'section': 'gram7-past-tense',\n",
       "   'correct': [],\n",
       "   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'SAYING', 'SAID')]},\n",
       "  {'section': 'gram8-plural',\n",
       "   'correct': [],\n",
       "   'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]},\n",
       "  {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []},\n",
       "  {'section': 'Total accuracy',\n",
       "   'correct': [('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN')],\n",
       "   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "    ('HE', 'SHE', 'HIS', 'HER'),\n",
       "    ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "    ('HIS', 'HER', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "    ('MAN', 'WOMAN', 'HIS', 'HER'),\n",
       "    ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "    ('SMALL', 'SMALLER', 'LOW', 'LOWER'),\n",
       "    ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
       "    ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "    ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "    ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "    ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "    ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'),\n",
       "    ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "    ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "    ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "    ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "    ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "    ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf73ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
